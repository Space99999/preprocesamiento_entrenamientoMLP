{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d62d3c07-0054-43bc-be19-a0969ce19768",
   "metadata": {},
   "source": [
    "## FASE 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f6206b-b089-4fbd-a8bd-ff093efc3dff",
   "metadata": {},
   "source": [
    "### Contar desbalance de clases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f6cc77b-4660-4de9-8022-84cf2c90f129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando archivo C:/U Octavo Semestre/tesis/Preprocesamiento de datos dataset actualizado/DATASET ACTUALIZADO V2\\Friday-02-03-2018.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\josue\\AppData\\Local\\Temp\\ipykernel_18104\\2719318449.py:38: DeprecationWarning: `pl.count()` is deprecated. Please use `pl.len()` instead.\n",
      "(Deprecated in version 0.20.5)\n",
      "  total_rows = lazy_df.select(pl.count()).collect().item()\n",
      "C:\\Users\\josue\\AppData\\Local\\Temp\\ipykernel_18104\\2719318449.py:54: DeprecationWarning: `pl.count()` is deprecated. Please use `pl.len()` instead.\n",
      "(Deprecated in version 0.20.5)\n",
      "  .agg(pl.count().alias(\"count\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando archivo C:/U Octavo Semestre/tesis/Preprocesamiento de datos dataset actualizado/DATASET ACTUALIZADO V2\\Friday-23-02-2018.csv...\n",
      "Procesando archivo C:/U Octavo Semestre/tesis/Preprocesamiento de datos dataset actualizado/DATASET ACTUALIZADO V2\\Thursday-01-03-2018.csv...\n",
      "Procesando archivo C:/U Octavo Semestre/tesis/Preprocesamiento de datos dataset actualizado/DATASET ACTUALIZADO V2\\Thursday-22-02-2018.csv...\n",
      "Procesando archivo C:/U Octavo Semestre/tesis/Preprocesamiento de datos dataset actualizado/DATASET ACTUALIZADO V2\\Wednesday-28-02-2018.csv...\n",
      "\n",
      "Conteo total de etiquetas en todos los archivos:\n",
      "  Etiqueta BENIGN: 31245820 veces\n",
      "  Etiqueta Botnet Ares: 142921 veces\n",
      "  Etiqueta Botnet Ares - Attempted: 262 veces\n",
      "  Etiqueta Infiltration - Communication Victim Attacker: 204 veces\n",
      "  Etiqueta Infiltration - Dropbox Download: 85 veces\n",
      "  Etiqueta Infiltration - Dropbox Download - Attempted: 28 veces\n",
      "  Etiqueta Infiltration - NMAP Portscan: 89374 veces\n",
      "  Etiqueta Web Attack - Brute Force: 131 veces\n",
      "  Etiqueta Web Attack - Brute Force - Attempted: 137 veces\n",
      "  Etiqueta Web Attack - SQL: 39 veces\n",
      "  Etiqueta Web Attack - SQL - Attempted: 14 veces\n",
      "  Etiqueta Web Attack - XSS: 113 veces\n",
      "  Etiqueta Web Attack - XSS - Attempted: 4 veces\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import polars as pl\n",
    "from collections import defaultdict\n",
    "\n",
    "# Ruta de la carpeta que contiene los archivos CSV\n",
    "ruta_carpeta = \"C:/U Octavo Semestre/tesis/Preprocesamiento de datos dataset actualizado/DATASET ACTUALIZADO V2\"\n",
    "\n",
    "if not os.path.exists(ruta_carpeta):\n",
    "    raise Exception(f\"La carpeta {ruta_carpeta} no existe.\")\n",
    "\n",
    "archivos_csv = [os.path.join(ruta_carpeta, f) for f in os.listdir(ruta_carpeta) if f.endswith('.csv')]\n",
    "conteo_total = {}\n",
    "\n",
    "# Columnas conflictivas conocidas (puedes agregar m√°s si aparece otro error)\n",
    "columnas_conflictivas = [\n",
    "    \"TotLen Bwd Pkts\", \"Active Mean\", \"Active Std\", \"Dst Port\"\n",
    "]\n",
    "\n",
    "# Crear un dict para forzar tipos como Float64\n",
    "schema_overrides = {col: pl.Float64 for col in columnas_conflictivas}\n",
    "\n",
    "# Par√°metros de chunk\n",
    "chunk_size = 300_000\n",
    "\n",
    "for archivo in archivos_csv:\n",
    "    try:\n",
    "        print(f\"Procesando archivo {archivo}...\")\n",
    "        \n",
    "        # Leemos el archivo en \"lazy\" para no cargar todo en memoria\n",
    "        lazy_df = pl.scan_csv(\n",
    "            archivo,\n",
    "            infer_schema_length=10000,  # mayor para detectar correctamente los tipos\n",
    "            schema_overrides=schema_overrides,\n",
    "            ignore_errors=True          # omite filas con errores\n",
    "        )\n",
    "\n",
    "        # Obtenemos el n√∫mero total de filas del archivo\n",
    "        total_rows = lazy_df.select(pl.count()).collect().item()\n",
    "\n",
    "        if total_rows == 0:\n",
    "            print(f\"  El archivo {os.path.basename(archivo)} est√° vac√≠o.\")\n",
    "            continue\n",
    "\n",
    "        # Procesamos el archivo por chunks\n",
    "        for start_row in range(0, total_rows, chunk_size):\n",
    "            df_chunk = lazy_df.slice(start_row, chunk_size).collect()\n",
    "            if df_chunk.is_empty():\n",
    "                break\n",
    "\n",
    "            # Procesamos las etiquetas si existen\n",
    "            if \"Label\" in df_chunk.columns:\n",
    "                labels_count = (\n",
    "                    df_chunk.group_by(\"Label\")  # agrupamos por la columna 'Label'\n",
    "                    .agg(pl.count().alias(\"count\"))\n",
    "                    .sort(\"Label\")\n",
    "                )\n",
    "\n",
    "                for row in labels_count.iter_rows():\n",
    "                    label, count = row\n",
    "                    conteo_total[label] = conteo_total.get(label, 0) + count\n",
    "            else:\n",
    "                print(f\"  La columna 'Label' no se encuentra en el archivo {os.path.basename(archivo)}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error al procesar el archivo {archivo}:\\n  {e}\")\n",
    "\n",
    "# Mostrar conteo total\n",
    "print(\"\\nConteo total de etiquetas en todos los archivos:\")\n",
    "for label in sorted(conteo_total.keys()):\n",
    "    print(f\"  Etiqueta {label}: {conteo_total[label]} veces\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575d4c2c-595c-433c-aabc-ad03da262f28",
   "metadata": {},
   "source": [
    "### limitar la clase bening "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85311eb1-0e6b-4819-91b8-454867354a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import polars as pl\n",
    "\n",
    "# Carpetas\n",
    "input_folder = r\"C:/U Octavo Semestre/tesis/Preprocesamiento de datos dataset actualizado/DATASET ACTUALIZADO V2/\"\n",
    "output_folder = r\"C:/U Octavo Semestre/tesis/Preprocesamiento de datos dataset actualizado/dataset_undersampling/\"\n",
    "\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Par√°metros\n",
    "limite_benign_total = 500_000\n",
    "benign_por_archivo = 100_000\n",
    "benign_acumulado = 0\n",
    "\n",
    "archivos_csv = [f for f in os.listdir(input_folder) if f.endswith('.csv')]\n",
    "\n",
    "for archivo in archivos_csv:\n",
    "    print(f\"Procesando {archivo}...\")\n",
    "\n",
    "    # Leer archivo completo (Eager)\n",
    "    df = pl.read_csv(os.path.join(input_folder, archivo))\n",
    "\n",
    "    # Separar BENIGN y resto\n",
    "    benign_df = df.filter(pl.col(\"Label\") == \"BENIGN\")\n",
    "    resto_df = df.filter(pl.col(\"Label\") != \"BENIGN\")\n",
    "\n",
    "    # Limitar BENIGN a 60k por archivo, respetando l√≠mite global\n",
    "    if benign_acumulado < limite_benign_total:\n",
    "        remaining_global = limite_benign_total - benign_acumulado\n",
    "        take = min(benign_por_archivo, remaining_global, benign_df.shape[0])\n",
    "        # Selecci√≥n aleatoria de registros de la clase BENIGN\n",
    "        benign_keep = benign_df.sample(n=take, shuffle=True, seed=42) \n",
    "        benign_acumulado += benign_keep.shape[0]\n",
    "    else:\n",
    "        benign_keep = pl.DataFrame()\n",
    "\n",
    "    # Combinar BENIGN limitado + resto\n",
    "    df_final = pl.concat([benign_keep, resto_df], how=\"diagonal_relaxed\")\n",
    "\n",
    "    # Guardar archivo procesado en la nueva carpeta\n",
    "    output_path = os.path.join(output_folder, archivo)\n",
    "    df_final.write_csv(output_path)\n",
    "    print(f\"Guardado en: {output_path}\")\n",
    "    print(f\"BENIGN acumulado hasta ahora: {benign_acumulado}\")\n",
    "    print(f\"Filas procesadas de este archivo: {df_final.shape[0]}\")\n",
    "\n",
    "    # Liberar memoria antes de procesar el siguiente archivo\n",
    "    del df, benign_df, resto_df, benign_keep, df_final\n",
    "\n",
    "    # Si se alcanz√≥ el l√≠mite global de BENIGN, avisar\n",
    "    if benign_acumulado >= limite_benign_total:\n",
    "        print(\" L√≠mite global de BENIGN alcanzado.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca65696e-b5ec-4268-b01c-423cc477da07",
   "metadata": {},
   "source": [
    "### Correlaciones iniciales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a95a5b-02a3-421c-b33b-475532941f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import polars as pl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Ruta de tus archivos\n",
    "ruta_carpeta = r\"C:/U Octavo Semestre/tesis/Preprocesamiento de datos dataset actualizado/dataset_undersampling/\"\n",
    "\n",
    "if not os.path.exists(ruta_carpeta):\n",
    "    raise Exception(f\"La carpeta {ruta_carpeta} no existe.\")\n",
    "\n",
    "archivos_csv = [os.path.join(ruta_carpeta, f) for f in os.listdir(ruta_carpeta) if f.endswith('.csv')]\n",
    "\n",
    "# Cargar todos los CSV directamente en memoria\n",
    "dfs = []\n",
    "for archivo in archivos_csv:\n",
    "    print(f\"üîπ Cargando {archivo}...\")\n",
    "    df_temp = pl.read_csv(\n",
    "        archivo,\n",
    "        infer_schema_length=50000,\n",
    "        ignore_errors=True\n",
    "    )\n",
    "    dfs.append(df_temp)\n",
    "\n",
    "#  Concatenar todos los archivos\n",
    "df = pl.concat(dfs, how=\"diagonal_relaxed\")\n",
    "df = pl.DataFrame(df)\n",
    "del dfs\n",
    "\n",
    "print(\"\\n Datos cargados correctamente (sin l√≠mite en Benign)\")\n",
    "print(\"N√∫mero de filas:\", df.shape[0])\n",
    "print(\"N√∫mero de columnas:\", df.shape[1])\n",
    "\n",
    "#  Normalizar etiquetas\n",
    "label_mapping = {\n",
    "    \"Botnet Ares\": \"Bot\",\n",
    "    \"Web Attack - Brute Force\": \"WebAttacks\",\n",
    "    \"Web Attack - XSS\": \"WebAttacks\",\n",
    "    \"Infiltration - Communication Victim Attacker\": \"Infilteration\",\n",
    "    \"Infiltration - Dropbox Download\": \"Infilteration\",\n",
    "    \"Infiltration - NMAP Portscan\": \"Infilteration\",\n",
    "}\n",
    "\n",
    "df = df.with_columns([\n",
    "    pl.when(pl.col(\"Label\").str.to_lowercase() == \"benign\")\n",
    "      .then(pl.lit(\"benign\"))\n",
    "      .when(pl.col(\"Label\").str.contains(\"Attempted\"))\n",
    "      .then(pl.lit(\"benign\"))\n",
    "      .when(pl.col(\"Label\").is_in(list(label_mapping.keys())))\n",
    "      .then(\n",
    "          pl.col(\"Label\").replace_strict(\n",
    "              label_mapping, \n",
    "              return_dtype=pl.Utf8,\n",
    "              default=\"Other\"   # <- valores no mapeados van a \"Other\"\n",
    "          )\n",
    "      )\n",
    "      .otherwise(pl.col(\"Label\"))\n",
    "      .alias(\"Label\")\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "#  Eliminar etiquetas con \"SQL\"\n",
    "df = df.filter(~pl.col(\"Label\").str.contains(\"SQL\"))\n",
    "\n",
    "#  Conteo de etiquetas despu√©s de limpieza\n",
    "conteo_etiquetas = df.group_by(\"Label\").agg(pl.count().alias(\"count\")).sort(\"count\", descending=True)\n",
    "print(\"\\n Distribuci√≥n de clases finales:\\n\", conteo_etiquetas)\n",
    "\n",
    "# üîß Convertir columnas num√©ricas correctamente (excluyendo Label y campos no num√©ricos)\n",
    "excluir = ['id', 'Flow ID','Protocol',  'Src IP', 'Src Port', 'Dst IP', 'Dst Port', 'Timestamp', 'Label']\n",
    "for col in df.columns:\n",
    "    if col not in excluir:\n",
    "        df = df.with_columns([\n",
    "            pl.col(col)\n",
    "            .cast(pl.Utf8)\n",
    "            .str.replace(\",\", \"\")\n",
    "            .str.replace(r\"[Ee]\\+?\", \"e\", literal=False)\n",
    "            .cast(pl.Float64, strict=False)\n",
    "        ])\n",
    "\n",
    "\n",
    "\n",
    "#  Mapa de calor de correlaciones con columnas num√©ricas\n",
    "num_cols = [c for c in df.columns if df[c].dtype == pl.Float64]\n",
    "if len(num_cols) > 1:\n",
    "    corr = df.select(num_cols).to_pandas().corr()\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    plt.imshow(corr, cmap=\"coolwarm\", interpolation=\"nearest\")\n",
    "    plt.colorbar()\n",
    "    plt.xticks(range(len(corr.columns)), corr.columns, rotation=90, fontsize=5)\n",
    "    plt.yticks(range(len(corr.columns)), corr.columns, fontsize=5)\n",
    "    plt.title(\"Mapa de calor de correlaciones\", fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "#  Gr√°ficas de solo 2 features y 4 etiquetas ‚Üí 8 subplots\n",
    "features = [\"Flow Duration\", \"Flow Packets/s\"]\n",
    "orden_personal = [\"benign\",\"Bot\", \"Infilteration\", \"WebAttacks\"]\n",
    "labels_finales = [lab for lab in orden_personal if lab in df[\"Label\"].unique().to_list()]\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(2, len(labels_finales), figsize=(18, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for row, feature in enumerate(features):\n",
    "    for col, etiqueta in enumerate(labels_finales):\n",
    "        ax = axes[row * len(labels_finales) + col]\n",
    "        subset = df.filter(pl.col(\"Label\") == etiqueta)\n",
    "        if subset.shape[0] > 0:\n",
    "            n = min(100000, subset.shape[0])\n",
    "            subset_sample = subset.sample(n=n, seed=42)\n",
    "\n",
    "            valores = np.array(subset_sample[feature].to_list(), dtype=np.float64)\n",
    "            valores = valores[np.isfinite(valores)]\n",
    "\n",
    "            if len(valores) > 0:\n",
    "                ax.hist(valores, bins=50, alpha=0.7, color=\"steelblue\")\n",
    "\n",
    "        ax.set_title(f\"{etiqueta} - {feature}\")\n",
    "        ax.set_xlabel(\"Valor\")\n",
    "        ax.set_ylabel(\"Frecuencia\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "#  Nueva figura para 2 features ‚Üí 8 subplots\n",
    "features_nuevas = [\"Flow Bytes/s\", \"Average Packet Size\"]\n",
    "orden_personal = [\"benign\",\"Bot\", \"Infilteration\", \"WebAttacks\"]\n",
    "labels_finales = [lab for lab in orden_personal if lab in df[\"Label\"].unique().to_list()]\n",
    "\n",
    "fig, axes = plt.subplots(2, len(labels_finales), figsize=(18, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for row, feature in enumerate(features_nuevas):\n",
    "    for col, etiqueta in enumerate(labels_finales):\n",
    "        ax = axes[row * len(labels_finales) + col]\n",
    "        subset = df.filter(pl.col(\"Label\") == etiqueta)\n",
    "        if subset.shape[0] > 0:\n",
    "            n = min(50000, subset.shape[0])\n",
    "            subset_sample = subset.sample(n=n, seed=42)\n",
    "\n",
    "            valores = np.array(subset_sample[feature].to_list(), dtype=np.float64)\n",
    "            valores = valores[np.isfinite(valores)]  # elimina NaN, inf, -inf\n",
    "\n",
    "            if len(valores) > 0:\n",
    "                ax.hist(valores, bins=50, alpha=0.7, color=\"steelblue\")\n",
    "\n",
    "        ax.set_title(f\"{etiqueta} - {feature}\", fontsize=9)\n",
    "        ax.set_xlabel(\"Valor\", fontsize=8)\n",
    "        ax.set_ylabel(\"Frecuencia\", fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5471cf6a-1bd3-4536-a5ca-2d3c48a68955",
   "metadata": {},
   "source": [
    "### Verficar valores NaN, Nulos y duplicados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3e110aa-172f-4ef5-8668-01ace85195e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Analizando archivo: Friday-02-03-2018.csv\n",
      "   Total de filas: 243183\n",
      "   Valores NaN: 0\n",
      "   Valores Inf/-Inf: 0.0\n",
      "   Filas duplicadas: 0\n",
      "\n",
      " Analizando archivo: Friday-23-02-2018.csv\n",
      "   Total de filas: 100230\n",
      "   Valores NaN: 0\n",
      "   Valores Inf/-Inf: 0.0\n",
      "   Filas duplicadas: 0\n",
      "\n",
      " Analizando archivo: Thursday-01-03-2018.csv\n",
      "   Total de filas: 139847\n",
      "   Valores NaN: 0\n",
      "   Valores Inf/-Inf: 0.0\n",
      "   Filas duplicadas: 0\n",
      "\n",
      " Analizando archivo: Thursday-22-02-2018.csv\n",
      "   Total de filas: 100208\n",
      "   Valores NaN: 0\n",
      "   Valores Inf/-Inf: 0.0\n",
      "   Filas duplicadas: 0\n",
      "\n",
      " Analizando archivo: Wednesday-28-02-2018.csv\n",
      "   Total de filas: 149844\n",
      "   Valores NaN: 0\n",
      "   Valores Inf/-Inf: 0.0\n",
      "   Filas duplicadas: 0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "\n",
    "# Ruta de la carpeta con los CSV\n",
    "ruta_carpeta = r\"C:/U Octavo Semestre/tesis/Preprocesamiento de datos dataset actualizado/dataset_undersampling/\"\n",
    "\n",
    "if not os.path.exists(ruta_carpeta):\n",
    "    raise Exception(f\"La carpeta {ruta_carpeta} no existe.\")\n",
    "\n",
    "archivos_csv = [os.path.join(ruta_carpeta, f) for f in os.listdir(ruta_carpeta) if f.endswith('.csv')]\n",
    "\n",
    "# Columnas a excluir\n",
    "excluir = ['id', 'Flow ID', 'Protocol', 'Src IP', 'Src Port', 'Dst IP', 'Dst Port', 'Timestamp', 'Label']\n",
    "\n",
    "# Procesar cada archivo\n",
    "for archivo in archivos_csv:\n",
    "    print(f\"\\n Analizando archivo: {os.path.basename(archivo)}\")\n",
    "\n",
    "    # Cargar dataset\n",
    "    df = pl.read_csv(archivo)\n",
    "\n",
    "    # Asegurar que las columnas excluidas existen en el DataFrame\n",
    "    cols_excluir = [c for c in excluir if c in df.columns]\n",
    "\n",
    "    # Seleccionar columnas num√©ricas (excluyendo las de 'excluir')\n",
    "    cols_numericas = [c for c in df.columns if c not in cols_excluir]\n",
    "\n",
    "    # Convertir a Float64\n",
    "    df = df.with_columns([df[c].cast(pl.Float64, strict=False) for c in cols_numericas])\n",
    "\n",
    "    # Total de filas\n",
    "    total_filas = df.shape[0]\n",
    "\n",
    "    # Contar NaN\n",
    "    nan_count = df[cols_numericas].null_count().to_pandas().sum().sum()\n",
    "\n",
    "    # Contar Inf y -Inf\n",
    "    df_pandas = df[cols_numericas].to_pandas()\n",
    "    inf_count = df_pandas.replace([np.inf, -np.inf], np.nan).isna().sum().sum() - nan_count\n",
    "\n",
    "    # Contar duplicados (filas exactamente iguales)\n",
    "    duplicados = df.is_duplicated().sum()\n",
    "\n",
    "    print(f\"   Total de filas: {total_filas}\")\n",
    "    print(f\"   Valores NaN: {nan_count}\")\n",
    "    print(f\"   Valores Inf/-Inf: {inf_count}\")\n",
    "    print(f\"   Filas duplicadas: {duplicados}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11997d62-db31-4800-a368-0d9484de8515",
   "metadata": {},
   "source": [
    "## FASE 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03be686-17c4-4738-aad6-1c84896ab230",
   "metadata": {},
   "source": [
    "### Verificar los valores constantes en columas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5031b754-3552-45dd-ac58-053e54b3e22f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Analizando archivo: Friday-02-03-2018.csv\n",
      "   Columnas con todos ceros: ['Fwd URG Flags', 'Bwd URG Flags', 'URG Flag Count']\n",
      "   Columnas constantes: ['Fwd URG Flags', 'Bwd URG Flags', 'URG Flag Count']\n",
      "\n",
      " Analizando archivo: Friday-23-02-2018.csv\n",
      "   Columnas con todos ceros: ['Fwd URG Flags', 'Bwd URG Flags', 'URG Flag Count']\n",
      "   Columnas constantes: ['Fwd URG Flags', 'Bwd URG Flags', 'URG Flag Count']\n",
      "\n",
      " Analizando archivo: Thursday-01-03-2018.csv\n",
      "   Columnas con todos ceros: ['Bwd URG Flags']\n",
      "   Columnas constantes: ['Bwd URG Flags']\n",
      "\n",
      " Analizando archivo: Thursday-22-02-2018.csv\n",
      "   Columnas con todos ceros: ['Fwd URG Flags', 'Bwd URG Flags', 'URG Flag Count']\n",
      "   Columnas constantes: ['Fwd URG Flags', 'Bwd URG Flags', 'URG Flag Count']\n",
      "\n",
      " Analizando archivo: Wednesday-28-02-2018.csv\n",
      "   Columnas con todos ceros: ['Bwd URG Flags']\n",
      "   Columnas constantes: ['Bwd URG Flags']\n",
      "\n",
      " ==== RESULTADO GLOBAL ====\n",
      "Columnas siempre ceros en TODOS los archivos: ['Bwd URG Flags']\n",
      "Columnas siempre constantes en TODOS los archivos: ['Bwd URG Flags']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "\n",
    "# Ruta de la carpeta con los CSV\n",
    "ruta_carpeta = r\"C:/U Octavo Semestre/tesis/Preprocesamiento de datos dataset actualizado/dataset_undersampling/\"\n",
    "\n",
    "if not os.path.exists(ruta_carpeta):\n",
    "    raise Exception(f\"La carpeta {ruta_carpeta} no existe.\")\n",
    "\n",
    "archivos_csv = [os.path.join(ruta_carpeta, f) for f in os.listdir(ruta_carpeta) if f.endswith('.csv')]\n",
    "\n",
    "# Columnas a excluir\n",
    "excluir = ['id', 'Flow ID', 'Protocol', 'Src IP', 'Src Port', 'Dst IP', 'Dst Port', 'Timestamp', 'Label']\n",
    "\n",
    "# Listas globales para comparar entre archivos\n",
    "global_ceros = []\n",
    "global_constantes = []\n",
    "\n",
    "# Procesar cada archivo\n",
    "for archivo in archivos_csv:\n",
    "    print(f\"\\n Analizando archivo: {os.path.basename(archivo)}\")\n",
    "\n",
    "    # Cargar dataset\n",
    "    df = pl.read_csv(archivo)\n",
    "\n",
    "    # Asegurar que las columnas excluidas existen en el DataFrame\n",
    "    cols_excluir = [c for c in excluir if c in df.columns]\n",
    "\n",
    "    # Seleccionar columnas num√©ricas (excluyendo las de 'excluir')\n",
    "    cols_numericas = [c for c in df.columns if c not in cols_excluir]\n",
    "\n",
    "    # Convertir a Float64\n",
    "    df = df.with_columns([df[c].cast(pl.Float64, strict=False) for c in cols_numericas])\n",
    "\n",
    "    # Columnas con todos los valores en cero\n",
    "    cols_ceros = [c for c in cols_numericas if df[c].sum() == 0]\n",
    "\n",
    "    # Columnas constantes (un solo valor √∫nico en toda la columna)\n",
    "    cols_constantes = [c for c in cols_numericas if df[c].n_unique() == 1]\n",
    "\n",
    "    print(f\"   Columnas con todos ceros: {cols_ceros}\")\n",
    "    print(f\"   Columnas constantes: {cols_constantes}\")\n",
    "\n",
    "    # Guardar para an√°lisis global\n",
    "    global_ceros.append(set(cols_ceros))\n",
    "    global_constantes.append(set(cols_constantes))\n",
    "\n",
    "# ---- An√°lisis Global ----\n",
    "if global_ceros:\n",
    "    interseccion_ceros = set.intersection(*global_ceros)\n",
    "else:\n",
    "    interseccion_ceros = set()\n",
    "\n",
    "if global_constantes:\n",
    "    interseccion_constantes = set.intersection(*global_constantes)\n",
    "else:\n",
    "    interseccion_constantes = set()\n",
    "\n",
    "print(\"\\n ==== RESULTADO GLOBAL ====\")\n",
    "print(f\"Columnas siempre ceros en TODOS los archivos: {sorted(interseccion_ceros)}\")\n",
    "print(f\"Columnas siempre constantes en TODOS los archivos: {sorted(interseccion_constantes)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc40bc9-613d-4b46-b174-b60425da006a",
   "metadata": {},
   "source": [
    "### Seleccion de caracteristicas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a10d8564-2a4e-44c6-98c0-3e66abe9e99c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• CARGANDO DATOS...\n",
      "   Cargando: Friday-02-03-2018.csv\n",
      "   Cargando: Friday-23-02-2018.csv\n",
      "   Cargando: Thursday-01-03-2018.csv\n",
      "   Cargando: Thursday-22-02-2018.csv\n",
      "   Cargando: Wednesday-28-02-2018.csv\n",
      "‚úÖ Dataset cargado: (733312, 83)\n",
      "üìä Distribuci√≥n final de etiquetas:\n",
      "shape: (4, 2)\n",
      "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
      "‚îÇ Label         ‚îÜ count  ‚îÇ\n",
      "‚îÇ ---           ‚îÜ ---    ‚îÇ\n",
      "‚îÇ str           ‚îÜ u32    ‚îÇ\n",
      "‚ïû‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï°\n",
      "‚îÇ BENIGN        ‚îÜ 500445 ‚îÇ\n",
      "‚îÇ WebAttacks    ‚îÜ 244    ‚îÇ\n",
      "‚îÇ Bot           ‚îÜ 142921 ‚îÇ\n",
      "‚îÇ Infilteration ‚îÜ 89663  ‚îÇ\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
      "‚úÖ Dataset final preparado: (733273, 83)\n",
      "üöÄ AN√ÅLISIS DE SELECCI√ìN DE CARACTER√çSTICAS CICIDS 2018\n",
      "============================================================\n",
      "‚úì Columna 'Label' movida al final como target\n",
      "‚úì Label Encoding aplicado a columna 'Protocol'\n",
      "  Protocolos √∫nicos codificados: 4\n",
      "‚úì Datos cargados: (733273, 83)\n",
      "‚úì Distribuci√≥n de clases:\n",
      "Label\n",
      "BENIGN           500445\n",
      "Bot              142921\n",
      "Infilteration     89663\n",
      "WebAttacks          244\n",
      "Name: count, dtype: int64\n",
      "\n",
      "üìä AN√ÅLISIS EXPLORATORIO\n",
      "========================================\n",
      "Shape: (733273, 83)\n",
      "Missing values: 0\n",
      "Classes: {'BENIGN': 500445, 'Bot': 142921, 'Infilteration': 89663, 'WebAttacks': 244}\n",
      "\n",
      "‚ö° SELECCI√ìN DE CARACTER√çSTICAS (HYBRID)\n",
      "==================================================\n",
      "\n",
      "üîç SELECCI√ìN AUTOM√ÅTICA DE CARACTER√çSTICAS\n",
      "=============================================\n",
      "üß™ Probando diferentes n√∫meros de caracter√≠sticas...\n",
      "  10 features -> F1-Score: 0.9975 (¬±0.0011)\n",
      "  15 features -> F1-Score: 0.9980 (¬±0.0010)\n",
      "  20 features -> F1-Score: 0.9983 (¬±0.0009)\n",
      "  25 features -> F1-Score: 0.9985 (¬±0.0009)\n",
      "  30 features -> F1-Score: 0.9987 (¬±0.0009)\n",
      "  35 features -> F1-Score: 0.9987 (¬±0.0009)\n",
      "  40 features -> F1-Score: 0.9987 (¬±0.0009)\n",
      "  45 features -> F1-Score: 0.9987 (¬±0.0009)\n",
      "\n",
      "üéØ N√öMERO √ìPTIMO: 30 caracter√≠sticas (F1: 0.9987)\n",
      "\n",
      "üîπ Selecci√≥n Univariada (k=30)...\n",
      "üîπ Recursive Feature Elimination (k=30)...\n",
      "üîπ Feature Importance (k=30)...\n",
      "\n",
      "‚úÖ Caracter√≠sticas seleccionadas: 30\n",
      "üèÜ Top 10 caracter√≠sticas:\n",
      "   1. Flow Duration\n",
      "   2. Bwd Packet Length Max\n",
      "   3. Bwd Packet Length Mean\n",
      "   4. Bwd Packet Length Std\n",
      "   5. Flow Packets/s\n",
      "   6. Flow IAT Max\n",
      "   7. Bwd IAT Total\n",
      "   8. Fwd Packets/s\n",
      "   9. Bwd Packets/s\n",
      "  10. FIN Flag Count\n",
      "üíæ Caracter√≠sticas guardadas en 'selected_features_auto.txt'\n",
      "\n",
      "‚úÖ AN√ÅLISIS COMPLETADO!\n",
      "   üìÅ Archivo guardado: selected_features_auto.txt\n",
      "   üî¢ Caracter√≠sticas seleccionadas: 30\n",
      "   üéØ N√∫mero √≥ptimo encontrado: 30\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, RFE\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class CICIDSFeatureSelector:\n",
    "    def __init__(self):\n",
    "        self.df = None\n",
    "        self.selected_features = None\n",
    "        self.protocol_encoder = LabelEncoder()\n",
    "        \n",
    "    def load_data(self, df):\n",
    "        \"\"\"Cargar y preparar datos\"\"\"\n",
    "        self.df = df.copy()\n",
    "        \n",
    "        # Reordenar: mover Label al final como columna target\n",
    "        if 'Label' in self.df.columns:\n",
    "            label_col = self.df.pop('Label')\n",
    "            self.df['Label'] = label_col\n",
    "            print(f\"‚úì Columna 'Label' movida al final como target\")\n",
    "        \n",
    "        # Aplicar Label Encoding a la columna Protocol\n",
    "        if 'Protocol' in self.df.columns:\n",
    "            self.df['Protocol'] = self.protocol_encoder.fit_transform(self.df['Protocol'])\n",
    "            print(f\"‚úì Label Encoding aplicado a columna 'Protocol'\")\n",
    "            print(f\"  Protocolos √∫nicos codificados: {len(self.protocol_encoder.classes_)}\")\n",
    "        \n",
    "        print(f\"‚úì Datos cargados: {self.df.shape}\")\n",
    "        print(f\"‚úì Distribuci√≥n de clases:\")\n",
    "        print(self.df.iloc[:, -1].value_counts())\n",
    "        \n",
    "    def exploratory_analysis(self):\n",
    "        \"\"\"An√°lisis exploratorio conciso\"\"\"\n",
    "        print(\"\\nüìä AN√ÅLISIS EXPLORATORIO\")\n",
    "        print(\"=\" * 40)\n",
    "        \n",
    "        target_col = self.df.columns[-1]\n",
    "        class_dist = self.df[target_col].value_counts()\n",
    "        \n",
    "        print(f\"Shape: {self.df.shape}\")\n",
    "        print(f\"Missing values: {self.df.isnull().sum().sum()}\")\n",
    "        print(f\"Classes: {class_dist.to_dict()}\")\n",
    "        \n",
    "        return class_dist\n",
    "    \n",
    "    def find_optimal_features(self, X, y, max_features=50):\n",
    "        \"\"\"Encontrar n√∫mero √≥ptimo de caracter√≠sticas autom√°ticamente\"\"\"\n",
    "        print(\"\\nüîç SELECCI√ìN AUTOM√ÅTICA DE CARACTER√çSTICAS\")\n",
    "        print(\"=\" * 45)\n",
    "        \n",
    "        # Codificar target\n",
    "        le = LabelEncoder()\n",
    "        y_encoded = le.fit_transform(y)\n",
    "        \n",
    "        # Probar diferentes n√∫meros de caracter√≠sticas usando validaci√≥n cruzada\n",
    "        feature_counts = range(10, min(max_features, X.shape[1]), 5)\n",
    "        cv_scores = []\n",
    "        \n",
    "        print(\"üß™ Probando diferentes n√∫meros de caracter√≠sticas...\")\n",
    "        \n",
    "        rf = RandomForestClassifier(n_estimators=50, random_state=42, n_jobs=-1)\n",
    "        \n",
    "        for n_features in feature_counts:\n",
    "            # Seleccionar caracter√≠sticas con F-test\n",
    "            selector = SelectKBest(score_func=f_classif, k=n_features)\n",
    "            X_selected = selector.fit_transform(X, y_encoded)\n",
    "            \n",
    "            # Validaci√≥n cruzada\n",
    "            scores = cross_val_score(rf, X_selected, y_encoded, cv=3, scoring='f1_weighted', n_jobs=-1)\n",
    "            cv_scores.append(scores.mean())\n",
    "            \n",
    "            print(f\"  {n_features:2d} features -> F1-Score: {scores.mean():.4f} (¬±{scores.std():.4f})\")\n",
    "        \n",
    "        # Encontrar n√∫mero √≥ptimo\n",
    "        optimal_idx = np.argmax(cv_scores)\n",
    "        optimal_n_features = list(feature_counts)[optimal_idx]\n",
    "        optimal_score = cv_scores[optimal_idx]\n",
    "        \n",
    "        print(f\"\\nüéØ N√öMERO √ìPTIMO: {optimal_n_features} caracter√≠sticas (F1: {optimal_score:.4f})\")\n",
    "        \n",
    "        return optimal_n_features, feature_counts, cv_scores\n",
    "    \n",
    "    def feature_selection(self, method='hybrid'):\n",
    "        \"\"\"Selecci√≥n de caracter√≠sticas con n√∫mero √≥ptimo autom√°tico\"\"\"\n",
    "        print(f\"\\n‚ö° SELECCI√ìN DE CARACTER√çSTICAS ({method.upper()})\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        target_col = self.df.columns[-1]\n",
    "        feature_cols = [col for col in self.df.columns if col != target_col]\n",
    "        \n",
    "        X = self.df[feature_cols]\n",
    "        y = self.df[target_col]\n",
    "        le = LabelEncoder()\n",
    "        y_encoded = le.fit_transform(y)\n",
    "        \n",
    "        # Encontrar n√∫mero √≥ptimo autom√°ticamente\n",
    "        optimal_n, feature_counts, cv_scores = self.find_optimal_features(X, y)\n",
    "        \n",
    "        selected_features_dict = {}\n",
    "        \n",
    "        # M√©todo univariado con n√∫mero √≥ptimo\n",
    "        print(f\"\\nüîπ Selecci√≥n Univariada (k={optimal_n})...\")\n",
    "        selector_f = SelectKBest(score_func=f_classif, k=optimal_n)\n",
    "        selector_f.fit(X, y_encoded)\n",
    "        selected_f = X.columns[selector_f.get_support()].tolist()\n",
    "        \n",
    "        # M√©todo RFE con n√∫mero √≥ptimo\n",
    "        print(f\"üîπ Recursive Feature Elimination (k={optimal_n})...\")\n",
    "        rf_estimator = RandomForestClassifier(n_estimators=50, random_state=42, n_jobs=-1)\n",
    "        selector_rfe = RFE(estimator=rf_estimator, n_features_to_select=optimal_n)\n",
    "        selector_rfe.fit(X, y_encoded)\n",
    "        selected_rfe = X.columns[selector_rfe.get_support()].tolist()\n",
    "        \n",
    "        # Importancia Random Forest\n",
    "        print(f\"üîπ Feature Importance (k={optimal_n})...\")\n",
    "        rf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "        rf.fit(X, y_encoded)\n",
    "        feature_importance = list(zip(X.columns, rf.feature_importances_))\n",
    "        feature_importance.sort(key=lambda x: x[1], reverse=True)\n",
    "        selected_importance = [feat for feat, imp in feature_importance[:optimal_n]]\n",
    "        \n",
    "        selected_features_dict = {\n",
    "            'univariate': selected_f,\n",
    "            'rfe': selected_rfe,\n",
    "            'importance': selected_importance\n",
    "        }\n",
    "        \n",
    "        if method == 'hybrid':\n",
    "            # Combinar m√©todos - caracter√≠sticas que aparecen en al menos 2 m√©todos\n",
    "            all_features = set()\n",
    "            for features in selected_features_dict.values():\n",
    "                all_features.update(features)\n",
    "            \n",
    "            feature_counts = {}\n",
    "            for features in selected_features_dict.values():\n",
    "                for feat in features:\n",
    "                    feature_counts[feat] = feature_counts.get(feat, 0) + 1\n",
    "            \n",
    "            # Seleccionar caracter√≠sticas m√°s consensuadas\n",
    "            final_features = [feat for feat, count in feature_counts.items() if count >= 2]\n",
    "            \n",
    "            # Si no hay suficientes, tomar las m√°s frecuentes\n",
    "            if len(final_features) < optimal_n // 2:\n",
    "                sorted_features = sorted(feature_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "                final_features = [feat for feat, count in sorted_features[:optimal_n]]\n",
    "            \n",
    "            self.selected_features = final_features\n",
    "        else:\n",
    "            self.selected_features = selected_features_dict[method]\n",
    "        \n",
    "        print(f\"\\n‚úÖ Caracter√≠sticas seleccionadas: {len(self.selected_features)}\")\n",
    "        print(\"üèÜ Top 10 caracter√≠sticas:\")\n",
    "        for i, feat in enumerate(self.selected_features[:10], 1):\n",
    "            print(f\"  {i:2d}. {feat}\")\n",
    "        \n",
    "        return self.selected_features, optimal_n, cv_scores\n",
    "    \n",
    "    def save_selected_features(self, filepath=\"selected_features.txt\"):\n",
    "        \"\"\"Guardar caracter√≠sticas seleccionadas\"\"\"\n",
    "        if self.selected_features is None:\n",
    "            print(\"‚ö†Ô∏è No hay caracter√≠sticas seleccionadas para guardar.\")\n",
    "            return\n",
    "        \n",
    "        with open(filepath, \"w\") as f:\n",
    "            f.write(\"Caracter√≠sticas seleccionadas:\\n\")\n",
    "            f.write(\"=\" * 30 + \"\\n\")\n",
    "            for i, feat in enumerate(self.selected_features, 1):\n",
    "                f.write(f\"{i:2d}. {feat}\\n\")\n",
    "            f.write(f\"\\nTotal: {len(self.selected_features)} caracter√≠sticas\")\n",
    "        \n",
    "        print(f\"üíæ Caracter√≠sticas guardadas en '{filepath}'\")\n",
    "    \n",
    "    def run_feature_selection_analysis(self, df):\n",
    "        \"\"\"Ejecutar solo an√°lisis de selecci√≥n de caracter√≠sticas\"\"\"\n",
    "        print(\"üöÄ AN√ÅLISIS DE SELECCI√ìN DE CARACTER√çSTICAS CICIDS 2018\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # 1. Cargar y preparar datos\n",
    "        self.load_data(df)\n",
    "        \n",
    "        # 2. An√°lisis exploratorio\n",
    "        class_dist = self.exploratory_analysis()\n",
    "        \n",
    "        # 3. Selecci√≥n autom√°tica de caracter√≠sticas\n",
    "        selected_features, optimal_n, cv_scores = self.feature_selection(method='hybrid')\n",
    "        \n",
    "        # 4. Guardar caracter√≠sticas seleccionadas\n",
    "        self.save_selected_features(\"selected_features_auto.txt\")\n",
    "        \n",
    "        print(\"\\n‚úÖ AN√ÅLISIS COMPLETADO!\")\n",
    "        print(f\"   üìÅ Archivo guardado: selected_features_auto.txt\")\n",
    "        print(f\"   üî¢ Caracter√≠sticas seleccionadas: {len(selected_features)}\")\n",
    "        print(f\"   üéØ N√∫mero √≥ptimo encontrado: {optimal_n}\")\n",
    "        \n",
    "        return {\n",
    "            'selected_features': selected_features,\n",
    "            'optimal_n_features': optimal_n,\n",
    "            'cv_scores': cv_scores,\n",
    "            'class_distribution': class_dist\n",
    "        }\n",
    "\n",
    "\n",
    "# EJECUCI√ìN PRINCIPAL\n",
    "# Ruta de los CSV\n",
    "ruta_carpeta = r\"C:/U Octavo Semestre/tesis/Preprocesamiento de datos dataset actualizado/dataset_undersampling/\"\n",
    "archivos = [os.path.join(ruta_carpeta, f) for f in os.listdir(ruta_carpeta) if f.endswith(\".csv\")]\n",
    "\n",
    "# Columnas a eliminar\n",
    "exclude_cols = ['id', 'Flow ID', 'Src IP', 'Src Port', 'Dst IP', 'Dst Port', 'Timestamp', 'Bwd URG Flags']\n",
    "\n",
    "print(\"üì• CARGANDO DATOS...\")\n",
    "# Cargar y concatenar todos los CSV en un solo DataFrame\n",
    "dfs = []\n",
    "for archivo in archivos:\n",
    "    print(f\"   Cargando: {os.path.basename(archivo)}\")\n",
    "    df_chunk = pl.read_csv(archivo)\n",
    "    \n",
    "    # Eliminar columnas no deseadas si existen\n",
    "    cols_to_drop = [c for c in exclude_cols if c in df_chunk.columns]\n",
    "    if cols_to_drop:\n",
    "        df_chunk = df_chunk.drop(cols_to_drop)\n",
    "    \n",
    "    # Convertir todas las columnas a float64 excepto Label y Protocol\n",
    "    for c in df_chunk.columns:\n",
    "        if c not in [\"Label\", \"Protocol\"]:\n",
    "            df_chunk = df_chunk.with_columns(pl.col(c).cast(pl.Float64))\n",
    "    \n",
    "    dfs.append(df_chunk)\n",
    "\n",
    "# Concatenar todos los CSV\n",
    "df = pl.concat(dfs, rechunk=True)\n",
    "print(f\"‚úÖ Dataset cargado: {df.shape}\")\n",
    "\n",
    "# Normalizar etiquetas\n",
    "label_mapping = {\n",
    "    \"Botnet Ares\": \"Bot\",\n",
    "    \"Web Attack - Brute Force\": \"WebAttacks\",\n",
    "    \"Web Attack - XSS\": \"WebAttacks\",\n",
    "    \"Infiltration - Communication Victim Attacker\": \"Infilteration\",\n",
    "    \"Infiltration - Dropbox Download\": \"Infilteration\",\n",
    "    \"Infiltration - NMAP Portscan\": \"Infilteration\",\n",
    "}\n",
    "\n",
    "df = df.with_columns([\n",
    "    pl.when(pl.col(\"Label\").str.to_lowercase() == \"benign\")\n",
    "      .then(pl.lit(\"BENIGN\"))\n",
    "      .when(pl.col(\"Label\").str.contains(\"Attempted\"))\n",
    "      .then(pl.lit(\"BENIGN\"))\n",
    "      .when(pl.col(\"Label\").is_in(list(label_mapping.keys())))\n",
    "      .then(\n",
    "          pl.col(\"Label\").replace_strict(\n",
    "              label_mapping, \n",
    "              return_dtype=pl.Utf8,\n",
    "              default=\"Other\"\n",
    "          )\n",
    "      )\n",
    "      .otherwise(pl.col(\"Label\"))\n",
    "      .alias(\"Label\")\n",
    "])\n",
    "\n",
    "# Eliminar etiquetas con \"SQL\"\n",
    "df = df.filter(~pl.col(\"Label\").str.contains(\"SQL\"))\n",
    "\n",
    "print(f\"üìä Distribuci√≥n final de etiquetas:\")\n",
    "print(df['Label'].value_counts())\n",
    "\n",
    "# Convertir a Pandas para el an√°lisis\n",
    "df = df.to_pandas()\n",
    "print(f\"‚úÖ Dataset final preparado: {df.shape}\")\n",
    "\n",
    "# EJECUTAR AN√ÅLISIS DE CARACTER√çSTICAS\n",
    "selector = CICIDSFeatureSelector()\n",
    "results = selector.run_feature_selection_analysis(df=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e8fd55-896c-4716-a750-4e3c67e8dd47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7543be5-5541-4bf1-bafb-77a37ba280e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6f6a03-8be3-48c2-8cd4-393ef9d52b11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
